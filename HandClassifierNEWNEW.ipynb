{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c6f65b-40c6-4484-8327-ec8e9e2716c1",
   "metadata": {},
   "source": [
    "**Imported Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e52cb01-3a9b-4ba5-a913-39791cf55fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mclevesluna/anaconda3/envs/condaenv/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/mclevesluna/anaconda3/envs/condaenv/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <2D1B8D5C-7891-3680-9CF9-F771AE880676> /Users/mclevesluna/anaconda3/envs/condaenv/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in:     <1BD88466-7CFF-3C8A-ADD6-913C4DC2F6C3> /Users/mclevesluna/anaconda3/envs/condaenv/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torchvision.transforms import ToPILImage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08b54a-bfe7-4ec3-9427-f2dc06811866",
   "metadata": {},
   "source": [
    "**Load Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7062808-01ae-4b1e-b573-b0feb52c60a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load images from each folder (test and train)\n",
    "TrainImagePaths = \"./V3Data/train\"\n",
    "TestImagePaths = \"./V3Data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "696ee442-71bf-49ce-866b-f7ba7c2ba997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Trainimages_from_folder(folder):\n",
    "    TrainImages = []\n",
    "    TrainLabelsList=[]\n",
    "    for file in os.listdir(folder):\n",
    "        TrainImages.append(os.path.join(folder,file))\n",
    "        TrainLabelsList.append(int(file.split(os.path.sep)[-1][-5])-1)\n",
    "    return TrainImages, TrainLabelsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96dfb9d2-766d-4f09-8a87-71963de54d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Testimages_from_folder(folder):\n",
    "    TestImages = []\n",
    "    TestLabelsList=[]\n",
    "    for file in os.listdir(folder):\n",
    "        TestImages.append(os.path.join(folder,file))\n",
    "        TestLabelsList.append(int(file.split(os.path.sep)[-1][-5])-1)\n",
    "    return TestImages, TestLabelsList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1034649-1b8a-43aa-adb2-5fbef5edf372",
   "metadata": {},
   "source": [
    "**Transform Images and Prepare for Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57353a50-19d4-42db-8233-c0c3020fe676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.SmallestMaxSize(max_size=256),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=60, p=0.5),\n",
    "        A.RandomCrop(height=224, width=224),\n",
    "        A.RGBShift(r_shift_limit=5, g_shift_limit=5, b_shift_limit=5, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.SmallestMaxSize(max_size=256),\n",
    "        A.CenterCrop(height=224, width=224),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a4f12-aaf1-4699-858a-3075bcfaaaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainImages, TrainLabels = load_Trainimages_from_folder(TrainImagePaths)\n",
    "TestImages, TestLabels = load_Testimages_from_folder(TestImagePaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8fdb4c-9981-443e-987d-ca5409b05101",
   "metadata": {},
   "source": [
    "**Create Model Classess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a2590ef-7fd1-4e63-839c-86902b1c2837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,data_paths,labels,transform=None,mode='train'):\n",
    "         self.data=data_paths\n",
    "         self.labels=labels\n",
    "         self.transform=transform\n",
    "         self.mode=mode\n",
    "    def __len__(self):\n",
    "       return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_name = self.data[idx]\n",
    "        img = cv2.imread(img_name)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # img=Image.fromarray(img)\n",
    "        # if self.transform is not None:\n",
    "        img = self.transform(image=img)[\"image\"]/255.\n",
    "        img = img.cuda()\n",
    "        \n",
    "        labels = torch.tensor(self.labels[idx]).cuda()\n",
    "\n",
    "        return img, labels\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fcddf4-7cdc-45ea-95aa-1e26d3c42370",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=ImageDataset(data_paths=TrainImages,labels=TrainLabels,transform=train_transform)\n",
    "val_dataset=ImageDataset(data_paths=TestImages,labels=TestLabels,transform=val_transform)\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "val_loader=DataLoader(val_dataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2f24b0f-585d-48a1-8bba-ccb05efd23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing attention layer\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_features_l, in_features_g, attn_features, up_factor, normalize_attn=True):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.up_factor = up_factor\n",
    "        self.normalize_attn = normalize_attn\n",
    "        self.W_l = nn.Conv2d(in_channels=in_features_l, out_channels=attn_features, kernel_size=1, padding=0, bias=False)\n",
    "        self.W_g = nn.Conv2d(in_channels=in_features_g, out_channels=attn_features, kernel_size=1, padding=0, bias=False)\n",
    "        self.phi = nn.Conv2d(in_channels=attn_features, out_channels=1, kernel_size=1, padding=0, bias=True)\n",
    "    def forward(self, l, g):\n",
    "        N, C, W, H = l.size()\n",
    "        l_ = self.W_l(l)\n",
    "        g_ = self.W_g(g)\n",
    "        if self.up_factor > 1:\n",
    "            g_ = F.interpolate(g_, scale_factor=self.up_factor, mode='bilinear', align_corners=False)\n",
    "        c = self.phi(F.relu(l_ + g_)) # batch_sizex1xWxH\n",
    "        \n",
    "        # compute attn map\n",
    "        if self.normalize_attn:\n",
    "            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,W,H)\n",
    "        else:\n",
    "            a = torch.sigmoid(c)\n",
    "        # re-weight the local feature\n",
    "        f = torch.mul(a.expand_as(l), l) # batch_sizexCxWxH\n",
    "        if self.normalize_attn:\n",
    "            output = f.view(N,C,-1).sum(dim=2) # weighted sum\n",
    "        else:\n",
    "            output = F.adaptive_avg_pool2d(f, (1,1)).view(N,C) # global average pooling\n",
    "        return a, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91b3dc82-6dcc-4a60-b98c-b0b23b7854c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnVGG(nn.Module):\n",
    "    def __init__(self, num_classes, normalize_attn=False, dropout=None):\n",
    "        super(AttnVGG, self).__init__()\n",
    "        net = models.vgg16_bn(pretrained=True)\n",
    "        self.conv_block1 = nn.Sequential(*list(net.features.children())[0:6])\n",
    "        self.conv_block2 = nn.Sequential(*list(net.features.children())[7:13])\n",
    "        self.conv_block3 = nn.Sequential(*list(net.features.children())[14:23])\n",
    "        self.conv_block4 = nn.Sequential(*list(net.features.children())[24:33])\n",
    "        self.conv_block5 = nn.Sequential(*list(net.features.children())[34:43])\n",
    "        self.pool = nn.AvgPool2d(7, stride=1)\n",
    "        self.dpt = None\n",
    "        if dropout is not None:\n",
    "            self.dpt = nn.Dropout(dropout)\n",
    "        self.cls = nn.Linear(in_features=512+512+256, out_features=num_classes, bias=True)\n",
    "        \n",
    "       # initialize the attention blocks defined above\n",
    "        self.attn1 = AttentionBlock(256, 512, 256, 4, normalize_attn=normalize_attn)\n",
    "        self.attn2 = AttentionBlock(512, 512, 256, 2, normalize_attn=normalize_attn)\n",
    "        \n",
    "       \n",
    "        self.reset_parameters(self.cls)\n",
    "        self.reset_parameters(self.attn1)\n",
    "        self.reset_parameters(self.attn2)\n",
    "    def reset_parameters(self, module):\n",
    "        for m in module.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1.)\n",
    "                nn.init.constant_(m.bias, 0.)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0., 0.01)\n",
    "                nn.init.constant_(m.bias, 0.)\n",
    "    def forward(self, x):\n",
    "        block1 = self.conv_block1(x)       # /1\n",
    "        pool1 = F.max_pool2d(block1, 2, 2) # /2\n",
    "        block2 = self.conv_block2(pool1)   # /2\n",
    "        pool2 = F.max_pool2d(block2, 2, 2) # /4\n",
    "        block3 = self.conv_block3(pool2)   # /4\n",
    "        pool3 = F.max_pool2d(block3, 2, 2) # /8\n",
    "        block4 = self.conv_block4(pool3)   # /8\n",
    "        pool4 = F.max_pool2d(block4, 2, 2) # /16\n",
    "        block5 = self.conv_block5(pool4)   # /16\n",
    "        pool5 = F.max_pool2d(block5, 2, 2) # /32\n",
    "        N, __, __, __ = pool5.size()\n",
    "        \n",
    "        g = self.pool(pool5).view(N,512)\n",
    "        a1, g1 = self.attn1(pool3, pool5)\n",
    "        a2, g2 = self.attn2(pool4, pool5)\n",
    "        g_hat = torch.cat((g,g1,g2), dim=1) # batch_size x C\n",
    "        if self.dpt is not None:\n",
    "            g_hat = self.dpt(g_hat)\n",
    "        out = self.cls(g_hat)\n",
    "\n",
    "        return [out, a1, a2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8537915a-bfd9-4368-acea-26456e7e087c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mclevesluna/anaconda3/envs/condaenv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/mclevesluna/anaconda3/envs/condaenv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = AttnVGG(num_classes=5, normalize_attn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cba8b0-dd2b-4280-ad72-6b2f6877d7ee",
   "metadata": {},
   "source": [
    "**Adjust Loss and Optimizer for Created Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f9bb914-e86c-4cb8-8b24-14e428d223a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changed focal to cross enthropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436ca93-8245-4e7e-ad18-a4311dfc0e87",
   "metadata": {},
   "source": [
    "**Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c933884-6ce7-4d76-81ea-72519616d4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20810227132425074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23015078\\AppData\\Local\\Temp\\ipykernel_17636\\3574433960.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  res = F.softmax(y_val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6071600172473909 tensor(0.7516, device='cuda:0')\n",
      "0.18783490977636197\n",
      "0.5145018588495898 tensor(0.7908, device='cuda:0')\n",
      "0.24882987986614064\n",
      "0.5870162542130238 tensor(0.7778, device='cuda:0')\n",
      "0.17253897143755018\n",
      "0.5104692158185773 tensor(0.8105, device='cuda:0')\n",
      "0.1527851317077875\n",
      "0.5158046749633511 tensor(0.8431, device='cuda:0')\n",
      "0.15163079690097309\n",
      "0.5469482316775551 tensor(0.7974, device='cuda:0')\n",
      "0.1257767251170263\n",
      "0.4739392288455289 tensor(0.8366, device='cuda:0')\n",
      "0.18556353431649325\n",
      "0.6216215875632818 tensor(0.7843, device='cuda:0')\n",
      "0.17946387972773575\n",
      "0.8399625208374917 tensor(0.6993, device='cuda:0')\n",
      "0.17238183115131972\n",
      "0.6467660231412284 tensor(0.7778, device='cuda:0')\n",
      "0.12173517025643732\n",
      "0.5325753948791256 tensor(0.7974, device='cuda:0')\n",
      "0.11708921021441134\n",
      "0.5075109527138932 tensor(0.7974, device='cuda:0')\n",
      "0.10205929751348931\n",
      "0.5654188533012262 tensor(0.7908, device='cuda:0')\n",
      "0.16200134944079853\n",
      "0.45296109472310137 tensor(0.8431, device='cuda:0')\n",
      "0.13793422018245952\n",
      "0.5960501220649461 tensor(0.7516, device='cuda:0')\n",
      "0.10438988900675279\n",
      "0.45727052616284175 tensor(0.8105, device='cuda:0')\n",
      "0.11451949819740725\n",
      "0.5077075762248117 tensor(0.7647, device='cuda:0')\n",
      "0.1534786358839128\n",
      "0.6486382118054952 tensor(0.7255, device='cuda:0')\n",
      "0.1221584889174598\n",
      "0.5224834850645362 tensor(0.7908, device='cuda:0')\n",
      "0.14517397304042812\n",
      "0.5435961518022749 tensor(0.7974, device='cuda:0')\n",
      "0.10666517547627048\n",
      "0.4729156803820599 tensor(0.8366, device='cuda:0')\n",
      "0.08367242764045553\n",
      "0.48623472119352834 tensor(0.8301, device='cuda:0')\n",
      "0.10515976897100122\n",
      "0.5661952448430551 tensor(0.7778, device='cuda:0')\n",
      "0.08881397876979374\n",
      "0.5020773691867837 tensor(0.7974, device='cuda:0')\n",
      "0.0958511805070973\n",
      "0.5241224933522175 tensor(0.8039, device='cuda:0')\n",
      "0.0477293411769518\n",
      "0.4208264233654036 tensor(0.8366, device='cuda:0')\n",
      "0.08703541705702864\n",
      "0.48523175886838266 tensor(0.8170, device='cuda:0')\n",
      "0.09236031970599802\n",
      "0.674745637608473 tensor(0.7451, device='cuda:0')\n",
      "0.15772464671512929\n",
      "0.587189923052743 tensor(0.7712, device='cuda:0')\n",
      "0.10547693968727821\n",
      "0.5781665044559738 tensor(0.7843, device='cuda:0')\n",
      "0.12155460432262682\n",
      "0.6111970351814152 tensor(0.7974, device='cuda:0')\n",
      "0.09335145522363303\n",
      "0.536525946742021 tensor(0.8105, device='cuda:0')\n",
      "0.10923745110630989\n",
      "0.4041233754312023 tensor(0.8693, device='cuda:0')\n",
      "0.07757619984175373\n",
      "0.6115309682931885 tensor(0.7712, device='cuda:0')\n",
      "0.07125199120491743\n",
      "0.4643247513921499 tensor(0.8431, device='cuda:0')\n",
      "0.05952643516768769\n",
      "0.5050652213478339 tensor(0.8497, device='cuda:0')\n",
      "0.05942773746281135\n",
      "0.4554748668843649 tensor(0.8366, device='cuda:0')\n",
      "0.08171308663014959\n",
      "0.5341957066542152 tensor(0.7843, device='cuda:0')\n",
      "0.05021241846176364\n",
      "0.4243877014321019 tensor(0.8366, device='cuda:0')\n",
      "0.04748088031121325\n",
      "0.36167365988565064 tensor(0.8627, device='cuda:0')\n",
      "0.08546828405886162\n",
      "0.48282740862120943 tensor(0.8366, device='cuda:0')\n",
      "0.07765700785647624\n",
      "0.48125545365364575 tensor(0.8431, device='cuda:0')\n",
      "0.0470531866431418\n",
      "0.854054797531693 tensor(0.6863, device='cuda:0')\n",
      "0.1330960430318444\n",
      "0.5158903292445709 tensor(0.8301, device='cuda:0')\n",
      "0.05400402720154422\n",
      "0.6496435300271283 tensor(0.7451, device='cuda:0')\n",
      "0.08548491790027517\n",
      "0.7388738386676909 tensor(0.7320, device='cuda:0')\n",
      "0.09887118283204915\n",
      "0.459891964474486 tensor(0.8170, device='cuda:0')\n",
      "0.07829725383440168\n",
      "0.4441996404484182 tensor(0.8235, device='cuda:0')\n",
      "0.0636589108489272\n",
      "0.5281487527844386 tensor(0.7974, device='cuda:0')\n",
      "0.09314125889298938\n",
      "0.5156076620836982 tensor(0.8170, device='cuda:0')\n",
      "0.05661937778406754\n",
      "0.5133204632303662 tensor(0.7908, device='cuda:0')\n",
      "0.0791191180895378\n",
      "0.5207182310819869 tensor(0.8105, device='cuda:0')\n",
      "0.06859738389938706\n",
      "0.6568731618995681 tensor(0.7778, device='cuda:0')\n",
      "0.05079372391309135\n",
      "0.5017335483503035 tensor(0.8039, device='cuda:0')\n",
      "0.07182247387018145\n",
      "0.4541873523738734 tensor(0.8366, device='cuda:0')\n",
      "0.04467122130118674\n",
      "0.4086555985625821 tensor(0.8431, device='cuda:0')\n",
      "0.05664999822222787\n",
      "0.5284127219531798 tensor(0.8431, device='cuda:0')\n",
      "0.027123678313772673\n",
      "0.7228070276142996 tensor(0.7712, device='cuda:0')\n",
      "0.04128562908892224\n",
      "0.3986829155462557 tensor(0.8627, device='cuda:0')\n",
      "0.04433348686888632\n",
      "0.4523136353714078 tensor(0.8366, device='cuda:0')\n",
      "0.044430417258565016\n",
      "0.3758811875089723 tensor(0.8627, device='cuda:0')\n",
      "0.034716044802491255\n",
      "0.36751166928568674 tensor(0.8824, device='cuda:0')\n",
      "0.03988231183597591\n",
      "0.3830650090688864 tensor(0.8627, device='cuda:0')\n",
      "0.047539990017667594\n",
      "0.4790971603962057 tensor(0.8693, device='cuda:0')\n",
      "0.03557614502269865\n",
      "0.6189835105263293 tensor(0.8170, device='cuda:0')\n",
      "0.10938974229119174\n",
      "0.3830374978704141 tensor(0.8824, device='cuda:0')\n",
      "0.0981274426960182\n",
      "0.3811902181782571 tensor(0.8693, device='cuda:0')\n",
      "0.08665781790140743\n",
      "0.49090087623906087 tensor(0.8431, device='cuda:0')\n",
      "0.0498883181376519\n",
      "0.4968789369590714 tensor(0.8366, device='cuda:0')\n",
      "0.08042317363092812\n",
      "0.712484616473518 tensor(0.7451, device='cuda:0')\n",
      "0.06670982043099839\n",
      "0.915162654092925 tensor(0.7190, device='cuda:0')\n",
      "0.054244723063098584\n",
      "0.6682693957057446 tensor(0.7451, device='cuda:0')\n",
      "0.04364840691432175\n",
      "0.7358782026635976 tensor(0.7582, device='cuda:0')\n",
      "0.04724809723883504\n",
      "0.6278867437698258 tensor(0.7974, device='cuda:0')\n",
      "0.054411518097877865\n",
      "0.5697871849052699 tensor(0.7908, device='cuda:0')\n",
      "0.0681514331031727\n",
      "0.5498875044069399 tensor(0.8039, device='cuda:0')\n",
      "0.05167235860532922\n",
      "0.47989229487599133 tensor(0.8235, device='cuda:0')\n",
      "0.027379148648256753\n",
      "0.41615393843533766 tensor(0.8431, device='cuda:0')\n",
      "0.03983134287125545\n",
      "0.4998250843829999 tensor(0.8170, device='cuda:0')\n",
      "0.02763727935678439\n",
      "0.47917270496543923 tensor(0.8301, device='cuda:0')\n",
      "0.03772774998598346\n",
      "0.5676479423118092 tensor(0.8039, device='cuda:0')\n",
      "0.043715324538203364\n",
      "0.4919441938102572 tensor(0.8235, device='cuda:0')\n",
      "0.04706340228071118\n",
      "0.5951531732685444 tensor(0.7843, device='cuda:0')\n",
      "0.03217838828667726\n",
      "0.7752122478189634 tensor(0.7190, device='cuda:0')\n",
      "0.05139362624925902\n",
      "0.5226960836179643 tensor(0.8105, device='cuda:0')\n",
      "0.03243223623736057\n",
      "0.4335509934496847 tensor(0.8627, device='cuda:0')\n",
      "0.022028289248360457\n",
      "0.5044069017271825 tensor(0.8235, device='cuda:0')\n",
      "0.024359715641078698\n",
      "0.5340493105293337 tensor(0.8105, device='cuda:0')\n",
      "0.03576150530858374\n",
      "0.4886354835080234 tensor(0.8366, device='cuda:0')\n",
      "0.024937297799624503\n",
      "0.4916901296610793 tensor(0.8366, device='cuda:0')\n",
      "0.059388054248013696\n",
      "0.5179104325792638 tensor(0.8366, device='cuda:0')\n",
      "0.033250908089074785\n",
      "0.6717563974070918 tensor(0.7647, device='cuda:0')\n",
      "0.08424366479342031\n",
      "0.6392724779541026 tensor(0.7843, device='cuda:0')\n",
      "0.0651949557714255\n",
      "0.474325920890455 tensor(0.8497, device='cuda:0')\n",
      "0.10879050740949446\n",
      "0.8183206603377281 tensor(0.7190, device='cuda:0')\n",
      "0.11710208028600347\n",
      "0.548387754609994 tensor(0.8170, device='cuda:0')\n",
      "0.05564894808865175\n",
      "0.5685410958036075 tensor(0.7974, device='cuda:0')\n",
      "0.06393121981925172\n",
      "0.335392416164384 tensor(0.8889, device='cuda:0')\n",
      "0.05365755159703151\n",
      "0.4089033180296884 tensor(0.8627, device='cuda:0')\n",
      "0.05444963153156384\n",
      "0.42989076125558034 tensor(0.8824, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "train_losses = []\n",
    "train_auc=[]\n",
    "val_auc=[]\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    train_preds=[]\n",
    "    train_targets=[]\n",
    "    auc_train=[]\n",
    "    loss_epoch_train=[]\n",
    "    running_loss = 0\n",
    "    running_loss_val = 0\n",
    "    # Run the training batches\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        \n",
    "        b+=1\n",
    "        y_pred,_,_=model(X_train)\n",
    "        loss = criterion(y_pred, y_train)   \n",
    "        loss_epoch_train.append(loss.item())\n",
    "        # For plotting purpose\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(running_loss/len(train_loader))   \n",
    "    acc = 0\n",
    "    # Run the testing batches\n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, y_test) in enumerate(val_loader):\n",
    "            \n",
    "            y_val,_,_ = model(X_test)\n",
    "            loss = criterion(y_val, y_test)\n",
    "            running_loss_val += loss.item()\n",
    "\n",
    "            res = F.softmax(y_val)\n",
    "\n",
    "            pred = torch.argmax(res, 1)\n",
    "            acc += torch.sum(pred==y_test)\n",
    "\n",
    "    if acc/len(val_loader)>=0.88\n",
    "        break\n",
    "\n",
    "    print(running_loss_val/len(val_loader), acc/len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5106bb1b-fde8-4222-86b1-3126007e04a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), os.path.join(save_dir, \"model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6922443-8943-4ffa-a52b-2044dd4bb8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttnVGG(\n",
       "  (conv_block1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_block2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_block3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_block4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv_block5): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (cls): Linear(in_features=1280, out_features=5, bias=True)\n",
       "  (attn1): AttentionBlock(\n",
       "    (W_l): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (W_g): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (phi): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (attn2): AttentionBlock(\n",
       "    (W_l): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (W_g): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (phi): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model on your CPU AND SET IT TO EVALUATION MODE\n",
    "model_path = \"/Users/mclevesluna/Documents/ML&AI/Henshim/V3SavedModel/model.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50ce0415-ea2e-4e3d-8c5b-ce9a6771dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image captured successfully!\n"
     ]
    }
   ],
   "source": [
    "#Take Picture\n",
    "# Initialize the camera\n",
    "cam_port = 0\n",
    "cam = cv2.VideoCapture(cam_port)\n",
    "\n",
    "while True:\n",
    "    # Read the input from the camera\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # Display the captured frame\n",
    "    cv2.imshow('Press q to capture', frame)\n",
    "\n",
    "    # Check for the 'q' key to capture the frame\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        # Save the captured frame as an image\n",
    "        cv2.imwrite('captured_image.png', frame)\n",
    "        print(\"Image captured successfully!\")\n",
    "        \n",
    "        # Release the camera and close the window\n",
    "        cam.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "# Release the camera and close the window (additional safety measure)\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13a2b73e-5961-44ef-a69f-e594ead48cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 0\n",
      "Probabilities: [[1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction for an image!\n",
    "def predict(input_path, model, transform):\n",
    "    # Load the saved image\n",
    "    input_image = cv2.imread(input_path)\n",
    "    if input_image is None:\n",
    "        print(\"Error: Unable to load the image from the specified path.\")\n",
    "        return None, None\n",
    "\n",
    "    input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Apply the validation transform\n",
    "    input_transformed = transform(image=input_image)['image']\n",
    "    input_transformed = input_transformed.unsqueeze(0)  # Makes the shape of the tensor [1, 3, 224, 224]\n",
    "\n",
    "    # Ensure input tensor data type matches the model's parameters\n",
    "    input_tensor = input_transformed.to(torch.float32)\n",
    "\n",
    "    # Convert the transformed input to a tensor and move it to GPU if available\n",
    "    input_tensor = input_tensor.cuda() if torch.cuda.is_available() else input_tensor\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        # Make prediction\n",
    "        output, _, _ = model(input_tensor)\n",
    "        # Convert the output tensor to a probability distribution\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        # Get the predicted class index\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "    return predicted_class, probabilities.cpu().numpy()\n",
    "\n",
    "# Example usage\n",
    "input_path = 'captured_image.png'\n",
    "predicted_class, probabilities = predict(input_path, model, val_transform)\n",
    "print(f'Predicted Class: {predicted_class}')\n",
    "print(f'Probabilities: {probabilities}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "592f74e3-cf6e-496a-b073-0b568d85691d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.9.18)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Predicted Class: 0\n",
      "Probabilities: [[1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "\n",
    "# Define the sound files corresponding to each class label\n",
    "sound_files = {\n",
    "    0: 'sound0.wav',\n",
    "    1: 'sound1.wav',\n",
    "    2: 'sound2.wav',\n",
    "    3: 'sound3.wav',\n",
    "    4: 'sound4.wav'\n",
    "}\n",
    "\n",
    "# Load the sounds\n",
    "sounds = {label: pygame.mixer.Sound(sound_file) for label, sound_file in sound_files.items()}\n",
    "\n",
    "# Make a prediction for an image!\n",
    "def predict(input_path, model, transform):\n",
    "    # Load the saved image\n",
    "    input_image = cv2.imread(input_path)\n",
    "    if input_image is None:\n",
    "        print(\"Error: Unable to load the image from the specified path.\")\n",
    "        return None, None\n",
    "\n",
    "    input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Apply the validation transform\n",
    "    input_transformed = transform(image=input_image)['image']\n",
    "    input_transformed = input_transformed.unsqueeze(0)  # Makes the shape of the tensor [1, 3, 224, 224]\n",
    "\n",
    "    # Ensure input tensor data type matches the model's parameters\n",
    "    input_tensor = input_transformed.to(torch.float32)\n",
    "\n",
    "    # Convert the transformed input to a tensor and move it to GPU if available\n",
    "    input_tensor = input_tensor.cuda() if torch.cuda.is_available() else input_tensor\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        # Make prediction\n",
    "        output, _, _ = model(input_tensor)\n",
    "        # Convert the output tensor to a probability distribution\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        # Get the predicted class index\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "    # Play the corresponding sound\n",
    "    sounds[predicted_class].play()\n",
    "\n",
    "    return predicted_class, probabilities.cpu().numpy()\n",
    "\n",
    "# Example usage\n",
    "input_path = 'captured_image.png'\n",
    "predicted_class, probabilities = predict(input_path, model, val_transform)\n",
    "print(f'Predicted Class: {predicted_class}')\n",
    "print(f'Probabilities: {probabilities}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e23db253-fb38-4bf0-8252-a50d764b996e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for volume commands (say 'up' or 'down')...\n",
      "Speech recognition could not understand audio.\n"
     ]
    }
   ],
   "source": [
    "# Initialize pygame\n",
    "pygame.init()\n",
    "\n",
    "# Define the sound files corresponding to each class label\n",
    "sound_files = {\n",
    "    0: 'sound0.wav',\n",
    "    1: 'sound1.wav',\n",
    "    2: 'sound2.wav',\n",
    "    3: 'sound3.wav',\n",
    "    4: 'sound4.wav'\n",
    "}\n",
    "\n",
    "# Load the sounds\n",
    "sounds = {label: pygame.mixer.Sound(sound_file) for label, sound_file in sound_files.items()}\n",
    "\n",
    "# Function to adjust volume based on speech command\n",
    "def adjust_volume():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening for volume commands (say 'up' or 'down')...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "    try:\n",
    "        command = recognizer.recognize_google(audio).lower()\n",
    "        if 'up' in command:\n",
    "            pygame.mixer.music.set_volume(min(pygame.mixer.music.get_volume() + 0.2, 1.0))\n",
    "            print(\"Volume increased.\")\n",
    "        elif 'down' in command:\n",
    "            pygame.mixer.music.set_volume(max(pygame.mixer.music.get_volume() - 0.2, 0.0))\n",
    "            print(\"Volume decreased.\")\n",
    "        else:\n",
    "            print(\"Unknown command. Please say 'up' or 'down'.\")\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Speech recognition could not understand audio.\")\n",
    "    except sr.RequestError as e:\n",
    "        print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "\n",
    "# Function to make a prediction and play sound with adjusted volume\n",
    "def predict_and_play(input_path, model, transform):\n",
    "    # Make a prediction\n",
    "    predicted_class, _ = predict(input_path, model, transform)\n",
    "    # Play sound with adjusted volume\n",
    "    sounds[predicted_class].set_volume(pygame.mixer.music.get_volume())\n",
    "    sounds[predicted_class].play()\n",
    "\n",
    "# Example usage\n",
    "input_path = 'captured_image.png'\n",
    "predict_and_play(input_path, model, val_transform)\n",
    "\n",
    "# Adjust volume based on speech command\n",
    "adjust_volume()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c43e97-b22e-4727-8510-09370619ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to start recording\n",
    "# Initialize the camera\n",
    "cam_port = 0\n",
    "cam = cv2.VideoCapture(cam_port)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('captured_video.avi', fourcc, 20.0, (640, 480))\n",
    "\n",
    "# Start capturing video\n",
    "while True:\n",
    "    # Read the input from the camera\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # Display the captured frame\n",
    "    cv2.imshow('Recording... Press q to stop', frame)\n",
    "\n",
    "    # Write the frame to the video file\n",
    "    out.write(frame)\n",
    "\n",
    "    # Check for the 'q' key to stop recording\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"Recording stopped!\")\n",
    "        break\n",
    "\n",
    "# Release the camera and video writer, and close the window\n",
    "cam.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4efeb71-055e-4af5-bdab-ad914ac284e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Classes: []\n",
      "Probabilities: []\n"
     ]
    }
   ],
   "source": [
    "def predict_from_video(video_path, model, transform):\n",
    "    # Initialize video capture object\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Initialize variables to store predictions\n",
    "    predictions = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    # Loop through video frames\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert frame to RGB\n",
    "        input_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply transformation\n",
    "        input_transformed = transform(image=input_image)['image']\n",
    "        input_transformed = input_transformed.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Ensure input tensor data type matches the model's parameters\n",
    "        input_tensor = input_transformed.to(torch.float32)\n",
    "        \n",
    "        # Move tensor to GPU if available\n",
    "        input_tensor = input_tensor.cuda() if torch.cuda.is_available() else input_tensor\n",
    "        \n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            output, _, _ = model(input_tensor)\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "        \n",
    "        # Append predictions and probabilities\n",
    "        predictions.append(predicted_class)\n",
    "        all_probabilities.append(probabilities.cpu().numpy())\n",
    "    \n",
    "    # Release video capture object\n",
    "    cap.release()\n",
    "    \n",
    "    return predictions, all_probabilities\n",
    "\n",
    "# Example usage\n",
    "video_path = 'captured_video.avi'  # Replace with the path to your captured video\n",
    "predicted_classes, all_probabilities = predict_from_video(video_path, model, val_transform)\n",
    "print(f'Predicted Classes: {predicted_classes}')\n",
    "print(f'Probabilities: {all_probabilities}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1992e676-7077-4ccc-93f5-0b0dc870bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction function for live webcam feed\n",
    "def predict_from_webcam(model, transform):\n",
    "    # Initialize video capture object for webcam\n",
    "    cap = cv2.VideoCapture(0)  # Use 0 as the argument for the default webcam\n",
    "    \n",
    "    # Define class labels\n",
    "    class_labels = ['0', '1', '2', '3', '4']  # Class labels corresponding to class indices 0, 1, 2, 3, 4\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert frame to RGB\n",
    "        input_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply transformation\n",
    "        input_transformed = transform(image=input_image)['image']\n",
    "        input_transformed = input_transformed.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Ensure input tensor data type matches the model's parameters\n",
    "        input_tensor = input_transformed.to(torch.float32)\n",
    "        \n",
    "        # Move tensor to GPU if available\n",
    "        input_tensor = input_tensor.cuda() if torch.cuda.is_available() else input_tensor\n",
    "        \n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            output, _, _ = model(input_tensor)\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "        \n",
    "        # Map predicted class to human-readable label\n",
    "        predicted_label = class_labels[predicted_class]\n",
    "        \n",
    "        # Display the frame with prediction\n",
    "        cv2.putText(frame, f'Predicted Class: {predicted_label}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.imshow('Webcam Feed', frame)\n",
    "        \n",
    "        # Break the loop if 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release video capture object and close all OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "predict_from_webcam(model, val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b1f5d3-48e8-47dc-a179-b6d8c1d18548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f674a-bcc2-4859-bd08-09fa0faeb713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
